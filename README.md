This research focuses on path planning in a grid-world environment, where an agent navigates from a start point to a goal while avoiding micro and macro obstacles using deep reinforcement learning (DRL) algorithms. By implementing and comparing three state-of-the-art algorithms—Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), and Double Deep Q-Network (DDQN)—the study evaluates their performance in terms of learning efficiency, obstacle avoidance, and goal-oriented navigation. The grid-world scenario, characterized by discrete state spaces and varying obstacle complexities, provides a structured benchmark for assessing each algorithm's capabilities. The results highlight the strengths and trade-offs of each approach, contributing to advancements in adaptive and efficient autonomous navigation systems for real-world applications.

 PPO is a reliable algorithm for continuous action spaces, its performance is limited when the problem is discretized. DDQN, being specifically designed for discrete action spaces, could not cope well with the complexity of the environment. On the other hand, SAC proved to be the most effective algorithm in this setup, achieving significantly higher rewards due to its enhanced exploration capabilities and entropy regularization, which enabled it to find optimal paths in the given environment. Thus, for this environment, SAC is the most suitable choice.
